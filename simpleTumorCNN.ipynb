{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import ConcatDataset, DataLoader, SubsetRandomSampler\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from skopt import gp_minimize, load\n",
        "from skopt.space import Real\n",
        "from skopt.callbacks import CheckpointSaver\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import re\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "# Checkpoint Configuration Variables\n",
        "# ==========================================\n",
        "CHECKPOINT_BASE_NAME = '3d_cv_optimization'\n",
        "USE_CHECKPOINT = False   # Set to True to resume from a checkpoint, False to start new\n",
        "DESIRED_CHECKPOINT_ID = None  # Set to None for latest, or an integer for a specific checkpoint ID\n",
        "USE_NARROWED_SPACE = True   # Set to True to use narrowed_search_space from the analysis cell (5b)\n",
        "\n",
        "# Local checkpoint directory\n",
        "DRIVE_DIR = r\"c:\\Users\\JMN\\Documents\\Privat\\Uddannelse\\ActiveML\\mini-projekt\\BO_Checkpoints\"\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# 1. Data Preprocessing & Loading\n",
        "# ==========================================\n",
        "# Local dataset path\n",
        "dataset_path = r\"c:\\Users\\JMN\\Documents\\Privat\\Uddannelse\\ActiveML\\mini-projekt\\dataset\"\n",
        "print(f\"Contents of {dataset_path}: {os.listdir(dataset_path)}\")\n",
        "\n",
        "# --- Compute (or load cached) dataset-specific normalization statistics ---\n",
        "import json\n",
        "NORM_STATS_FILE = os.path.join(DRIVE_DIR, \"dataset_norm_stats.json\")\n",
        "\n",
        "if os.path.exists(NORM_STATS_FILE):\n",
        "    with open(NORM_STATS_FILE, \"r\") as f:\n",
        "        _stats = json.load(f)\n",
        "    DATASET_MEAN = _stats[\"mean\"]\n",
        "    DATASET_STD  = _stats[\"std\"]\n",
        "    print(f\"Loaded cached normalization stats from {NORM_STATS_FILE}\")\n",
        "else:\n",
        "    print(\"Computing dataset-specific normalization statistics (first run)...\")\n",
        "    _tmp_transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    _tmp_train = ImageFolder(os.path.join(dataset_path, \"Training\"), transform=_tmp_transform)\n",
        "    _tmp_test  = ImageFolder(os.path.join(dataset_path, \"Testing\"),  transform=_tmp_transform)\n",
        "    _tmp_all   = ConcatDataset([_tmp_train, _tmp_test])\n",
        "    _tmp_loader = DataLoader(_tmp_all, batch_size=256, shuffle=False, num_workers=0)\n",
        "\n",
        "    _mean = torch.zeros(3)\n",
        "    _std  = torch.zeros(3)\n",
        "    _n_pixels = 0\n",
        "    for imgs, _ in tqdm(_tmp_loader, desc=\"Norm stats\", leave=False):\n",
        "        b, c, h, w = imgs.shape\n",
        "        _n_pixels += b * h * w\n",
        "        _mean += imgs.sum(dim=[0, 2, 3])\n",
        "        _std  += (imgs ** 2).sum(dim=[0, 2, 3])\n",
        "\n",
        "    DATASET_MEAN = (_mean / _n_pixels).tolist()\n",
        "    DATASET_STD  = ((_std / _n_pixels - torch.tensor(DATASET_MEAN) ** 2).sqrt()).tolist()\n",
        "    del _tmp_transform, _tmp_train, _tmp_test, _tmp_all, _tmp_loader, _mean, _std, _n_pixels\n",
        "\n",
        "    # Save for future runs\n",
        "    with open(NORM_STATS_FILE, \"w\") as f:\n",
        "        json.dump({\"mean\": DATASET_MEAN, \"std\": DATASET_STD}, f, indent=2)\n",
        "    print(f\"Saved normalization stats to {NORM_STATS_FILE}\")\n",
        "\n",
        "print(f\"Dataset mean: {DATASET_MEAN}\")\n",
        "print(f\"Dataset std:  {DATASET_STD}\")\n",
        "\n",
        "# --- Final transform with computed statistics ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=DATASET_MEAN, std=DATASET_STD)\n",
        "])\n",
        "\n",
        "training_dataset = ImageFolder(os.path.join(dataset_path, \"Training\"), transform=transform)\n",
        "testing_dataset  = ImageFolder(os.path.join(dataset_path, \"Testing\"),  transform=transform)\n",
        "dataset = ConcatDataset([training_dataset, testing_dataset])\n",
        "print(f\"Total dataset size: {len(dataset)} images\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Model Definition — SimpleTumorCNN\n",
        "# ==========================================\n",
        "class SimpleTumorCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight custom CNN (~24k parameters).\n",
        "    3 conv blocks with BatchNorm, AdaptiveAvgPool, and a single FC head.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=4, dropout_rate=0.1):\n",
        "        super(SimpleTumorCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: 3 -> 16 channels\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Block 2: 16 -> 32 channels\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Block 3: 32 -> 64 channels\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Global pooling\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Verify parameter count\n",
        "_tmp_model = SimpleTumorCNN(num_classes=4, dropout_rate=0.1)\n",
        "_param_count = sum(p.numel() for p in _tmp_model.parameters())\n",
        "print(f\"SimpleTumorCNN parameter count: {_param_count:,}\")\n",
        "del _tmp_model\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Training Params & BO Configuration\n",
        "# ==========================================\n",
        "CALLS = 18         # New BO trials (matches grid search budget; warm-start points are free)\n",
        "EPOCHS = 50        # Epochs per trial per fold\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 3\n",
        "N_FOLDS = 3        # 3-Fold Cross-Validation\n",
        "SEED = 42\n",
        "\n",
        "# 3D Search Space\n",
        "search_space = [\n",
        "    Real(1e-4, 1e-1, prior='log-uniform', name='learning_rate'),\n",
        "    Real(1e-5, 1e-2, prior='log-uniform', name='weight_decay'),\n",
        "    Real(0.0,  0.5,  prior='uniform',     name='dropout'),\n",
        "]\n",
        "\n",
        "# Global state for trial numbering and WandB grouping\n",
        "current_call = 0\n",
        "checkpoint_id_for_this_run = 0  # Will be set by main block; used as WandB group\n",
        "\n",
        "def get_checkpoint_id(base_name, find_latest=False):\n",
        "    \"\"\"\n",
        "    Generates a new unique ID for new runs or finds the latest existing ID for resuming.\n",
        "    \"\"\"\n",
        "    existing_ids = []\n",
        "    for f_name in os.listdir(DRIVE_DIR):\n",
        "        match = re.match(rf'^{re.escape(base_name)}_(\\d+)\\.pkl$', f_name)\n",
        "        if match:\n",
        "            existing_ids.append(int(match.group(1)))\n",
        "\n",
        "    if find_latest:\n",
        "        return max(existing_ids) if existing_ids else None\n",
        "    else:\n",
        "        if not existing_ids:\n",
        "            return 0\n",
        "        else:\n",
        "            existing_ids.sort()\n",
        "            for i, _id in enumerate(existing_ids):\n",
        "                if i != _id:\n",
        "                    return i\n",
        "            return len(existing_ids)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4. Objective Function (3-Fold CV)\n",
        "# ==========================================\n",
        "def train_model(params):\n",
        "    \"\"\"\n",
        "    Objective function for Bayesian Optimization.\n",
        "    Trains SimpleTumorCNN with 3-Fold CV and returns mean validation loss.\n",
        "    \"\"\"\n",
        "    global current_call, checkpoint_id_for_this_run\n",
        "    current_call += 1\n",
        "\n",
        "    learning_rate = params[0]\n",
        "    weight_decay  = params[1]\n",
        "    dropout       = params[2]\n",
        "\n",
        "    # Clear GPU memory from previous trial\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Initialize WandB for this trial\n",
        "    run = wandb.init(\n",
        "        entity=\"2121jmmn-danmarks-tekniske-universitet-dtu\",\n",
        "        project=\"3d_cv_simpleTumorCNN\",\n",
        "        group=f\"{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}\",\n",
        "        name=f\"trial_{current_call}\",\n",
        "        reinit=True,\n",
        "        resume=\"never\",\n",
        "        config={\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"dropout\": dropout,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"n_folds\": N_FOLDS,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"trial\": current_call,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  Trial {current_call}/{CALLS}\")\n",
        "    print(f\"  lr={learning_rate:.6f}  wd={weight_decay:.6f}  dropout={dropout:.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- 3-Fold Cross-Validation ---\n",
        "    kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_losses = []\n",
        "    fold_accuracies = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(range(len(dataset)))):\n",
        "        print(f\"\\n  --- Fold {fold_idx + 1}/{N_FOLDS} ---\")\n",
        "\n",
        "        # Samplers for this fold\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler   = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        workers = NUM_WORKERS\n",
        "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                  sampler=train_sampler,\n",
        "                                  num_workers=workers, persistent_workers=True)\n",
        "        val_loader   = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                  sampler=val_sampler,\n",
        "                                  num_workers=workers, persistent_workers=True)\n",
        "\n",
        "        # Fresh model & optimizer per fold\n",
        "        model = SimpleTumorCNN(num_classes=4, dropout_rate=dropout).to(device)\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "        # --- Training loop ---\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            data_time = 0.0\n",
        "            compute_time = 0.0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f\"  Fold {fold_idx+1} Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "            end = time.time()\n",
        "\n",
        "            for _batch_idx, (inputs, labels) in enumerate(pbar):\n",
        "                data_time += time.time() - end\n",
        "\n",
        "                comp_start = time.time()\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                compute_time += time.time() - comp_start\n",
        "\n",
        "                total_time = data_time + compute_time\n",
        "                data_pct = 100 * data_time / total_time if total_time > 0 else 0\n",
        "\n",
        "                elapsed = pbar.format_dict.get('elapsed', 0)\n",
        "                remaining = (pbar.format_dict.get('total', 1) - pbar.format_dict.get('n', 0)) \\\n",
        "                            * pbar.format_dict.get('elapsed', 0) \\\n",
        "                            / max(pbar.format_dict.get('n', 1), 1)\n",
        "                epoch_total = elapsed + remaining\n",
        "                et_min, et_sec = divmod(int(epoch_total), 60)\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'epoch_est': f'{et_min:02d}:{et_sec:02d}',\n",
        "                    'data%': f'{data_pct:.0f}%'\n",
        "                })\n",
        "                end = time.time()\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "            wandb.log({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": avg_train_loss,\n",
        "                \"data_loading_pct\": data_pct,\n",
        "            })\n",
        "\n",
        "        # --- Validation for this fold ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_fold_val_loss = val_loss / len(val_loader)\n",
        "        fold_accuracy = 100 * correct / total\n",
        "        fold_losses.append(avg_fold_val_loss)\n",
        "        fold_accuracies.append(fold_accuracy)\n",
        "\n",
        "        wandb.log({\n",
        "            \"fold\": fold_idx + 1,\n",
        "            \"fold_val_loss\": avg_fold_val_loss,\n",
        "            \"fold_val_accuracy\": fold_accuracy,\n",
        "        })\n",
        "        print(f\"  Fold {fold_idx+1} — Val Loss: {avg_fold_val_loss:.4f}, Accuracy: {fold_accuracy:.2f}%\")\n",
        "\n",
        "        # Cleanup per fold\n",
        "        del model, optimizer, train_loader, val_loader\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # --- Average across folds ---\n",
        "    mean_val_loss = float(np.mean(fold_losses))\n",
        "    mean_accuracy = float(np.mean(fold_accuracies))\n",
        "\n",
        "    wandb.log({\n",
        "        \"mean_cv_val_loss\": mean_val_loss,\n",
        "        \"mean_cv_val_accuracy\": mean_accuracy,\n",
        "    })\n",
        "\n",
        "    print(f\"\\n  Trial {current_call} finished — Mean CV Loss: {mean_val_loss:.4f}, Mean Accuracy: {mean_accuracy:.2f}%\")\n",
        "    run.finish()\n",
        "\n",
        "    return mean_val_loss\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. Checkpoint Logic & Bayesian Optimization\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    x0 = None\n",
        "    y0 = None\n",
        "    current_call = 0\n",
        "    checkpoint_id_for_this_run = None\n",
        "    checkpoint_file = None\n",
        "\n",
        "    if USE_CHECKPOINT:\n",
        "        if DESIRED_CHECKPOINT_ID is not None:\n",
        "            checkpoint_id_for_this_run = DESIRED_CHECKPOINT_ID\n",
        "            checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "\n",
        "            if os.path.exists(checkpoint_file):\n",
        "                print(f\"Attempting to load specific checkpoint from {checkpoint_file}...\")\n",
        "                try:\n",
        "                    res_loaded = load(checkpoint_file)\n",
        "                    x0 = [list(xi) for xi in res_loaded.x_iters]  # Ensure list of lists\n",
        "                    y0 = list(res_loaded.func_vals)                # Ensure plain list\n",
        "                    current_call = len(x0)\n",
        "                    best_so_far = min(y0)\n",
        "                    print(f\"Resuming from {current_call} previous calls from ID {checkpoint_id_for_this_run}.\")\n",
        "                    print(f\"  Best loss so far: {best_so_far:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: Could not load checkpoint {checkpoint_file}: {e}. Starting new.\")\n",
        "                    checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                    checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                    print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "            else:\n",
        "                print(f\"ERROR: Checkpoint file {checkpoint_file} not found. Starting new optimization.\")\n",
        "                checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "        else:\n",
        "            latest_id = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=True)\n",
        "            if latest_id is not None:\n",
        "                checkpoint_id_for_this_run = latest_id\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Attempting to load latest checkpoint from {checkpoint_file}...\")\n",
        "                try:\n",
        "                    res_loaded = load(checkpoint_file)\n",
        "                    x0 = [list(xi) for xi in res_loaded.x_iters]\n",
        "                    y0 = list(res_loaded.func_vals)\n",
        "                    current_call = len(x0)\n",
        "                    best_so_far = min(y0)\n",
        "                    print(f\"Resuming from {current_call} previous calls from latest ID {checkpoint_id_for_this_run}.\")\n",
        "                    print(f\"  Best loss so far: {best_so_far:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: Could not load checkpoint {checkpoint_file}: {e}. Starting new.\")\n",
        "                    checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                    checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                    print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "            else:\n",
        "                print(\"No existing checkpoints found. Starting new optimization.\")\n",
        "                checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "    else:\n",
        "        print(\"USE_CHECKPOINT is False. Starting a brand new optimization.\")\n",
        "        checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "        checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "        print(f\"New optimization will use checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "    if checkpoint_file is None:\n",
        "        checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "        checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "\n",
        "    checkpoint_callback = CheckpointSaver(checkpoint_file)\n",
        "    remaining_calls = max(0, CALLS - current_call)\n",
        "\n",
        "    print(f\"Starting optimization with {remaining_calls} remaining calls (Total CALLS: {CALLS})...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    if remaining_calls > 0:\n",
        "        # Determine which search space to use\n",
        "        if USE_NARROWED_SPACE and 'narrowed_search_space' in dir() and narrowed_search_space is not None:\n",
        "            _active_space = narrowed_search_space\n",
        "            # Warm-start with points from narrowed analysis\n",
        "            # These are FREE data for the GP — they don't count against CALLS\n",
        "            if x0 is None and narrowed_x0:\n",
        "                x0 = narrowed_x0\n",
        "                y0 = narrowed_y0\n",
        "                # Do NOT update current_call or remaining_calls;\n",
        "                # warm-start points are prior knowledge, not new evaluations\n",
        "            print(f\"Using NARROWED search space with {len(x0) if x0 else 0} warm-start points.\")\n",
        "            print(f\"Will run {remaining_calls} NEW trials (warm-start points are free).\")\n",
        "        else:\n",
        "            _active_space = search_space\n",
        "            if USE_NARROWED_SPACE:\n",
        "                print(\"WARNING: USE_NARROWED_SPACE=True but narrowed_search_space not found. Using original.\")\n",
        "            print(f\"Using ORIGINAL search space.\")\n",
        "\n",
        "        # For narrowed space with warm-start: no random points needed, GP has data\n",
        "        # For original space: use some initial random exploration\n",
        "        if USE_NARROWED_SPACE and x0 is not None and len(x0) > 0:\n",
        "            required_random = 0\n",
        "        else:\n",
        "            required_random = max(0, 20 - len(x0 if x0 is not None else []))\n",
        "\n",
        "        res = gp_minimize(\n",
        "            train_model,\n",
        "            _active_space,                     # 3D: [lr, weight_decay, dropout]\n",
        "            acq_func=\"EI\",                     # Expected Improvement\n",
        "            xi=0.01,                          # Exploit within narrowed space\n",
        "            n_calls=remaining_calls,\n",
        "            n_initial_points=required_random,\n",
        "            noise=\"gaussian\",\n",
        "            random_state=SEED,\n",
        "            callback=[checkpoint_callback],\n",
        "            x0=x0,\n",
        "            y0=y0,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"All {CALLS} calls already completed based on loaded checkpoint.\")\n",
        "        if x0 is not None and y0 is not None:\n",
        "            best_idx = np.argmin(y0)\n",
        "            best_lr      = x0[best_idx][0]\n",
        "            best_wd      = x0[best_idx][1]\n",
        "            best_dropout = x0[best_idx][2]\n",
        "            best_loss    = y0[best_idx]\n",
        "\n",
        "            class MockResult:\n",
        "                def __init__(self, x, fun):\n",
        "                    self.x = x\n",
        "                    self.fun = fun\n",
        "\n",
        "            res = MockResult([best_lr, best_wd, best_dropout], best_loss)\n",
        "            print(f\"Best from checkpoint — LR: {res.x[0]:.6f}, WD: {res.x[1]:.6f}, \"\n",
        "                  f\"Dropout: {res.x[2]:.4f}, Loss: {res.fun:.4f}\")\n",
        "        else:\n",
        "            print(\"No results to display as no checkpoint was loaded and no new calls were made.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nOptimization finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
        "    if 'res' in locals():\n",
        "        print(f\"Best LR: {res.x[0]:.6f}, Best Weight Decay: {res.x[1]:.6f}, \"\n",
        "              f\"Best Dropout: {res.x[2]:.4f}, Best Loss: {res.fun:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 5b. Analyse BO Checkpoint → Narrowed Search Space\n",
        "# ==========================================\n",
        "# Load the BO checkpoint, extract the first N trials, find the top X\n",
        "# by mean_val_loss, compute their min/max per dimension, and define\n",
        "# a narrowed search space with a configurable margin.\n",
        "# The results are stored in `narrowed_search_space`, `narrowed_x0`,\n",
        "# `narrowed_y0` for use by the BO cell (set USE_NARROWED_SPACE = True).\n",
        "\n",
        "# --- Configuration ---\n",
        "ANALYSE_FIRST_N_TRIALS = 36   # Only look at the first N trials from the BO checkpoint\n",
        "TOP_X = 10                     # Number of best trials to base the narrowed space on\n",
        "MARGIN_PCT = 5                # % margin added above max and below min of each dimension\n",
        "\n",
        "# Original search space bounds (for clamping)\n",
        "ORIG_BOUNDS = {\n",
        "    'learning_rate': (1e-4, 1e-1),\n",
        "    'weight_decay':  (1e-5, 1e-2),\n",
        "    'dropout':       (0.0,  0.5),\n",
        "}\n",
        "\n",
        "# --- Load BO checkpoint ---\n",
        "_analyse_ckpt_file = None\n",
        "_analyse_latest_id = get_checkpoint_id('3d_cv_optimization', find_latest=True)\n",
        "\n",
        "if _analyse_latest_id is not None:\n",
        "    _analyse_ckpt_file = os.path.join(DRIVE_DIR, f\"3d_cv_optimization_{_analyse_latest_id}.pkl\")\n",
        "    print(f\"Loading BO checkpoint: {_analyse_ckpt_file}\")\n",
        "else:\n",
        "    print(\"ERROR: No BO checkpoint found. Run the BO cell first.\")\n",
        "\n",
        "if _analyse_ckpt_file and os.path.exists(_analyse_ckpt_file):\n",
        "    _res = load(_analyse_ckpt_file)\n",
        "    _all_x = [list(xi) for xi in _res.x_iters]\n",
        "    _all_y = list(_res.func_vals)\n",
        "    print(f\"Total trials in checkpoint: {len(_all_x)}\")\n",
        "\n",
        "    # Restrict to first N trials\n",
        "    _n = min(ANALYSE_FIRST_N_TRIALS, len(_all_x))\n",
        "    _x_subset = _all_x[:_n]\n",
        "    _y_subset = _all_y[:_n]\n",
        "    print(f\"Analysing first {_n} trials.\")\n",
        "\n",
        "    # Rank by loss and take top X\n",
        "    _ranked = sorted(zip(_y_subset, _x_subset, range(1, _n + 1)), key=lambda t: t[0])\n",
        "    _top = _ranked[:TOP_X]\n",
        "\n",
        "    print(f\"\\n{'='*74}\")\n",
        "    print(f\"  Top {len(_top)} trials (out of first {_n}) by mean CV val loss\")\n",
        "    print(f\"{'='*74}\")\n",
        "    print(f\"  {'Rank':<5} {'Trial#':<8} {'Loss':<10} {'LR':<14} {'WD':<14} {'Dropout':<10}\")\n",
        "    print(f\"  {'-'*5} {'-'*8} {'-'*10} {'-'*14} {'-'*14} {'-'*10}\")\n",
        "    for i, (loss, params, trial_num) in enumerate(_top):\n",
        "        print(f\"  {i+1:<5} {trial_num:<8} {loss:<10.4f} {params[0]:<14.6e} {params[1]:<14.6e} {params[2]:<10.4f}\")\n",
        "\n",
        "    # Extract per-dimension min/max from top X\n",
        "    _top_params = [p for _, p, _ in _top]\n",
        "    _top_lrs  = [p[0] for p in _top_params]\n",
        "    _top_wds  = [p[1] for p in _top_params]\n",
        "    _top_dos  = [p[2] for p in _top_params]\n",
        "\n",
        "    _raw_bounds = {\n",
        "        'learning_rate': (min(_top_lrs), max(_top_lrs)),\n",
        "        'weight_decay':  (min(_top_wds), max(_top_wds)),\n",
        "        'dropout':       (min(_top_dos), max(_top_dos)),\n",
        "    }\n",
        "\n",
        "    print(f\"\\n  Raw bounds from top {TOP_X}:\")\n",
        "    for dim, (lo, hi) in _raw_bounds.items():\n",
        "        scale = \"log\" if dim != \"dropout\" else \"lin\"\n",
        "        print(f\"    {dim:<16} [{lo:.6e}, {hi:.6e}]  ({scale})\")\n",
        "\n",
        "    # Apply margin — for log-uniform dims, margin is applied in log-space\n",
        "    margin_frac = MARGIN_PCT / 100.0\n",
        "\n",
        "    def _apply_margin(lo, hi, orig_lo, orig_hi, is_log=False):\n",
        "        \"\"\"Expand [lo, hi] by margin_frac, clamped to original bounds.\"\"\"\n",
        "        if is_log:\n",
        "            log_lo, log_hi = np.log10(lo), np.log10(hi)\n",
        "            log_range = log_hi - log_lo\n",
        "            log_range = max(log_range, 0.1)  # min 0.1 decades\n",
        "            new_log_lo = log_lo - margin_frac * log_range\n",
        "            new_log_hi = log_hi + margin_frac * log_range\n",
        "            new_lo = max(10 ** new_log_lo, orig_lo)\n",
        "            new_hi = min(10 ** new_log_hi, orig_hi)\n",
        "        else:\n",
        "            lin_range = hi - lo\n",
        "            lin_range = max(lin_range, 0.02)  # min range 0.02\n",
        "            new_lo = max(lo - margin_frac * lin_range, orig_lo)\n",
        "            new_hi = min(hi + margin_frac * lin_range, orig_hi)\n",
        "        return new_lo, new_hi\n",
        "\n",
        "    _narrowed_lr = _apply_margin(*_raw_bounds['learning_rate'], *ORIG_BOUNDS['learning_rate'], is_log=True)\n",
        "    _narrowed_wd = _apply_margin(*_raw_bounds['weight_decay'],  *ORIG_BOUNDS['weight_decay'],  is_log=True)\n",
        "    _narrowed_do = _apply_margin(*_raw_bounds['dropout'],       *ORIG_BOUNDS['dropout'],       is_log=False)\n",
        "\n",
        "    # --- Compare narrowed vs original bounds ---\n",
        "    def _pct_of_original(new_lo, new_hi, orig_lo, orig_hi, is_log=False):\n",
        "        \"\"\"Compute what % of the original range the narrowed range covers.\"\"\"\n",
        "        if is_log:\n",
        "            orig_range = np.log10(orig_hi) - np.log10(orig_lo)\n",
        "            new_range  = np.log10(new_hi)  - np.log10(new_lo)\n",
        "        else:\n",
        "            orig_range = orig_hi - orig_lo\n",
        "            new_range  = new_hi  - new_lo\n",
        "        return 100.0 * new_range / orig_range if orig_range > 0 else 0.0\n",
        "\n",
        "    _pct_lr = _pct_of_original(*_narrowed_lr, *ORIG_BOUNDS['learning_rate'], is_log=True)\n",
        "    _pct_wd = _pct_of_original(*_narrowed_wd, *ORIG_BOUNDS['weight_decay'],  is_log=True)\n",
        "    _pct_do = _pct_of_original(*_narrowed_do, *ORIG_BOUNDS['dropout'],       is_log=False)\n",
        "\n",
        "    print(f\"\\n  Narrowed bounds (with {MARGIN_PCT}% margin, clamped to original space):\")\n",
        "    print(f\"  {'Dimension':<16} {'Narrowed Range':<36} {'Original Range':<36} {'% of Original':<14}\")\n",
        "    print(f\"  {'-'*16} {'-'*36} {'-'*36} {'-'*14}\")\n",
        "    print(f\"  {'learning_rate':<16} [{_narrowed_lr[0]:.6e}, {_narrowed_lr[1]:.6e}]  (log)   \"\n",
        "          f\"[{ORIG_BOUNDS['learning_rate'][0]:.6e}, {ORIG_BOUNDS['learning_rate'][1]:.6e}]  (log)   \"\n",
        "          f\"{_pct_lr:>6.1f}%\")\n",
        "    print(f\"  {'weight_decay':<16} [{_narrowed_wd[0]:.6e}, {_narrowed_wd[1]:.6e}]  (log)   \"\n",
        "          f\"[{ORIG_BOUNDS['weight_decay'][0]:.6e}, {ORIG_BOUNDS['weight_decay'][1]:.6e}]  (log)   \"\n",
        "          f\"{_pct_wd:>6.1f}%\")\n",
        "    print(f\"  {'dropout':<16} [{_narrowed_do[0]:.6e}, {_narrowed_do[1]:.6e}]  (lin)   \"\n",
        "          f\"[{ORIG_BOUNDS['dropout'][0]:.6e}, {ORIG_BOUNDS['dropout'][1]:.6e}]  (lin)   \"\n",
        "          f\"{_pct_do:>6.1f}%\")\n",
        "\n",
        "    _total_vol_pct = _pct_lr * _pct_wd * _pct_do / (100 * 100)\n",
        "    print(f\"\\n  Combined volume: {_total_vol_pct:.1f}% of original search space\")\n",
        "\n",
        "    # --- Build the narrowed search space (same format as the original) ---\n",
        "    narrowed_search_space = [\n",
        "        Real(_narrowed_lr[0], _narrowed_lr[1], prior='log-uniform', name='learning_rate'),\n",
        "        Real(_narrowed_wd[0], _narrowed_wd[1], prior='log-uniform', name='weight_decay'),\n",
        "        Real(_narrowed_do[0], _narrowed_do[1], prior='uniform',     name='dropout'),\n",
        "    ]\n",
        "\n",
        "    # Collect evaluated points that fall within the narrowed bounds (for warm-starting BO)\n",
        "    narrowed_x0 = []\n",
        "    narrowed_y0 = []\n",
        "    for xi, yi in zip(_x_subset, _y_subset):\n",
        "        lr_ok = _narrowed_lr[0] <= xi[0] <= _narrowed_lr[1]\n",
        "        wd_ok = _narrowed_wd[0] <= xi[1] <= _narrowed_wd[1]\n",
        "        do_ok = _narrowed_do[0] <= xi[2] <= _narrowed_do[1]\n",
        "        if lr_ok and wd_ok and do_ok:\n",
        "            narrowed_x0.append(xi)\n",
        "            narrowed_y0.append(yi)\n",
        "\n",
        "    print(f\"\\n  Warm-start points inside narrowed space: {len(narrowed_x0)} / {_n}\")\n",
        "    if narrowed_x0:\n",
        "        _best_ws_idx = int(np.argmin(narrowed_y0))\n",
        "        print(f\"  Best warm-start point: loss={narrowed_y0[_best_ws_idx]:.4f}\")\n",
        "\n",
        "    print(f\"\\n  To use in the BO cell, set USE_NARROWED_SPACE = True\")\n",
        "    print(f\"  Variables available: narrowed_search_space, narrowed_x0, narrowed_y0\")\n",
        "\n",
        "    # Cleanup\n",
        "    del _res, _all_x, _all_y, _x_subset, _y_subset, _ranked, _top, _top_params\n",
        "    del _top_lrs, _top_wds, _top_dos, _raw_bounds\n",
        "\n",
        "else:\n",
        "    if _analyse_ckpt_file:\n",
        "        print(f\"ERROR: Checkpoint file {_analyse_ckpt_file} not found.\")\n",
        "    narrowed_search_space = None\n",
        "    narrowed_x0 = None\n",
        "    narrowed_y0 = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 5c. 3D Visualisation of BO Checkpoint\n",
        "# ==========================================\n",
        "# Interactive 3D scatter plot: LR vs WD vs Loss, coloured by loss.\n",
        "# Dropout is encoded as marker size (larger = higher dropout).\n",
        "# The top-X trials from the analysis cell are highlighted in red.\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# --- Load checkpoint (reuse the same logic as 5b) ---\n",
        "_viz_ckpt_id = get_checkpoint_id('3d_cv_optimization', find_latest=True)\n",
        "if _viz_ckpt_id is None:\n",
        "    raise RuntimeError(\"No BO checkpoint found. Run the BO cell first.\")\n",
        "\n",
        "_viz_ckpt_file = os.path.join(DRIVE_DIR, f\"3d_cv_optimization_{_viz_ckpt_id}.pkl\")\n",
        "_viz_res = load(_viz_ckpt_file)\n",
        "_viz_x = [list(xi) for xi in _viz_res.x_iters]\n",
        "_viz_y = list(_viz_res.func_vals)\n",
        "\n",
        "# Restrict to analysed subset if the variable exists\n",
        "_viz_n = min(ANALYSE_FIRST_N_TRIALS, len(_viz_x)) if 'ANALYSE_FIRST_N_TRIALS' in dir() else len(_viz_x)\n",
        "_viz_x = _viz_x[:_viz_n]\n",
        "_viz_y = _viz_y[:_viz_n]\n",
        "\n",
        "_viz_lr  = np.array([p[0] for p in _viz_x])\n",
        "_viz_wd  = np.array([p[1] for p in _viz_x])\n",
        "_viz_do  = np.array([p[2] for p in _viz_x])\n",
        "_viz_loss = np.array(_viz_y)\n",
        "\n",
        "# Scale dropout → marker size (min 4, max 18)\n",
        "_do_min, _do_max = _viz_do.min(), _viz_do.max()\n",
        "if _do_max > _do_min:\n",
        "    _viz_sizes = 4 + 14 * (_viz_do - _do_min) / (_do_max - _do_min)\n",
        "else:\n",
        "    _viz_sizes = np.full_like(_viz_do, 10.0)\n",
        "\n",
        "# Identify top-X indices (same TOP_X as 5b)\n",
        "_top_x_count = TOP_X if 'TOP_X' in dir() else 5\n",
        "_sorted_idx = np.argsort(_viz_loss)\n",
        "_top_idx = set(_sorted_idx[:_top_x_count])\n",
        "_is_top = np.array([i in _top_idx for i in range(len(_viz_loss))])\n",
        "\n",
        "# Hover text\n",
        "_hover = [\n",
        "    f\"Trial {i+1}<br>LR: {_viz_lr[i]:.6e}<br>WD: {_viz_wd[i]:.6e}<br>\"\n",
        "    f\"Dropout: {_viz_do[i]:.4f}<br>Loss: {_viz_loss[i]:.4f}\"\n",
        "    for i in range(len(_viz_loss))\n",
        "]\n",
        "\n",
        "# --- Build figure ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# All trials\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=np.log10(_viz_lr[~_is_top]),\n",
        "    y=np.log10(_viz_wd[~_is_top]),\n",
        "    z=_viz_loss[~_is_top],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=_viz_sizes[~_is_top],\n",
        "        color=_viz_loss[~_is_top],\n",
        "        colorscale='Viridis',\n",
        "        colorbar=dict(title='Loss', x=1.05),\n",
        "        opacity=0.6,\n",
        "        line=dict(width=0.5, color='white'),\n",
        "    ),\n",
        "    text=[h for h, t in zip(_hover, _is_top) if not t],\n",
        "    hoverinfo='text',\n",
        "    name='Trials',\n",
        "))\n",
        "\n",
        "# Top X trials (highlighted)\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=np.log10(_viz_lr[_is_top]),\n",
        "    y=np.log10(_viz_wd[_is_top]),\n",
        "    z=_viz_loss[_is_top],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=_viz_sizes[_is_top] + 4,\n",
        "        color='red',\n",
        "        opacity=0.9,\n",
        "        symbol='diamond',\n",
        "        line=dict(width=1, color='darkred'),\n",
        "    ),\n",
        "    text=[h for h, t in zip(_hover, _is_top) if t],\n",
        "    hoverinfo='text',\n",
        "    name=f'Top {_top_x_count}',\n",
        "))\n",
        "\n",
        "# If narrowed bounds exist, draw the narrowed bounding box\n",
        "if 'narrowed_search_space' in dir() and narrowed_search_space is not None:\n",
        "    _nb_lr = [np.log10(_narrowed_lr[0]), np.log10(_narrowed_lr[1])]\n",
        "    _nb_wd = [np.log10(_narrowed_wd[0]), np.log10(_narrowed_wd[1])]\n",
        "    _nb_z_lo = float(_viz_loss.min()) - 0.01\n",
        "    _nb_z_hi = float(_viz_loss.max()) + 0.01\n",
        "\n",
        "    # 12 edges of a rectangular box\n",
        "    def _box_edges(x0, x1, y0, y1, z0, z1):\n",
        "        edges_x, edges_y, edges_z = [], [], []\n",
        "        for (xa, ya, za), (xb, yb, zb) in [\n",
        "            ((x0,y0,z0),(x1,y0,z0)), ((x0,y1,z0),(x1,y1,z0)),\n",
        "            ((x0,y0,z1),(x1,y0,z1)), ((x0,y1,z1),(x1,y1,z1)),\n",
        "            ((x0,y0,z0),(x0,y1,z0)), ((x1,y0,z0),(x1,y1,z0)),\n",
        "            ((x0,y0,z1),(x0,y1,z1)), ((x1,y0,z1),(x1,y1,z1)),\n",
        "            ((x0,y0,z0),(x0,y0,z1)), ((x1,y0,z0),(x1,y0,z1)),\n",
        "            ((x0,y1,z0),(x0,y1,z1)), ((x1,y1,z0),(x1,y1,z1)),\n",
        "        ]:\n",
        "            edges_x += [xa, xb, None]\n",
        "            edges_y += [ya, yb, None]\n",
        "            edges_z += [za, zb, None]\n",
        "        return edges_x, edges_y, edges_z\n",
        "\n",
        "    bx, by, bz = _box_edges(_nb_lr[0], _nb_lr[1], _nb_wd[0], _nb_wd[1], _nb_z_lo, _nb_z_hi)\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=bx, y=by, z=bz,\n",
        "        mode='lines',\n",
        "        line=dict(color='orange', width=3),\n",
        "        name='Narrowed bounds',\n",
        "        hoverinfo='skip',\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f'BO Trials (first {_viz_n}) — Marker size ∝ Dropout',\n",
        "    scene=dict(\n",
        "        xaxis_title='log₁₀(Learning Rate)',\n",
        "        yaxis_title='log₁₀(Weight Decay)',\n",
        "        zaxis_title='Mean CV Val Loss',\n",
        "    ),\n",
        "    width=900,\n",
        "    height=700,\n",
        "    legend=dict(x=0.02, y=0.98),\n",
        "    margin=dict(l=0, r=0, b=0, t=40),\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Cleanup\n",
        "del _viz_res, _viz_x, _viz_y, _viz_lr, _viz_wd, _viz_do, _viz_loss\n",
        "del _viz_sizes, _sorted_idx, _top_idx, _is_top, _hover\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 6. Grid Search Baseline (3×3×2 = 18 trials)\n",
        "# ==========================================\n",
        "# This cell runs a full-factorial grid search over the same 3D\n",
        "# hyperparameter space used by BO, to serve as a comparison baseline.\n",
        "# LR and WD are log-spaced; dropout is linearly spaced.\n",
        "# Results are checkpointed to a JSON file after each trial for resume support.\n",
        "# All trials are grouped together in WandB under \"grid_search_{id}\".\n",
        "\n",
        "import itertools\n",
        "import json as _json  # alias to avoid shadowing the earlier import\n",
        "\n",
        "# --- Grid Search Configuration ---\n",
        "GRID_CHECKPOINT_BASE_NAME = 'grid_search'\n",
        "GRID_USE_CHECKPOINT = True  # Set to False to force a fresh grid search\n",
        "\n",
        "# Build the 3D grid (log-spaced for LR/WD, linear for dropout)\n",
        "lr_grid      = np.logspace(np.log10(1e-4), np.log10(1e-1), 3).tolist()   # 3 points\n",
        "wd_grid      = np.logspace(np.log10(1e-5), np.log10(1e-2), 3).tolist()   # 3 points\n",
        "dropout_grid = np.linspace(0.0, 0.5, 2).tolist()                          # 2 points\n",
        "\n",
        "GRID_TOTAL = len(lr_grid) * len(wd_grid) * len(dropout_grid)  # 18\n",
        "\n",
        "# Generate all combinations in a deterministic order\n",
        "grid_combinations = [\n",
        "    [lr, wd, do]\n",
        "    for lr, wd, do in itertools.product(lr_grid, wd_grid, dropout_grid)\n",
        "]\n",
        "\n",
        "print(f\"Grid Search: {len(lr_grid)} LR × {len(wd_grid)} WD × {len(dropout_grid)} Dropout = {GRID_TOTAL} trials\")\n",
        "print(f\"  LR grid  (log): {[f'{v:.6f}' for v in lr_grid]}\")\n",
        "print(f\"  WD grid  (log): {[f'{v:.6f}' for v in wd_grid]}\")\n",
        "print(f\"  Dropout  (lin): {[f'{v:.4f}' for v in dropout_grid]}\")\n",
        "\n",
        "\n",
        "# --- JSON checkpoint helpers ---\n",
        "def _get_grid_checkpoint_id(base_name, find_latest=False):\n",
        "    \"\"\"Find existing grid search JSON checkpoint IDs.\"\"\"\n",
        "    existing_ids = []\n",
        "    for f_name in os.listdir(DRIVE_DIR):\n",
        "        match = re.match(rf'^{re.escape(base_name)}_(\\d+)\\.json$', f_name)\n",
        "        if match:\n",
        "            existing_ids.append(int(match.group(1)))\n",
        "    if find_latest:\n",
        "        return max(existing_ids) if existing_ids else None\n",
        "    else:\n",
        "        if not existing_ids:\n",
        "            return 0\n",
        "        existing_ids.sort()\n",
        "        for i, _id in enumerate(existing_ids):\n",
        "            if i != _id:\n",
        "                return i\n",
        "        return len(existing_ids)\n",
        "\n",
        "\n",
        "def _load_grid_checkpoint(filepath):\n",
        "    \"\"\"Load completed results from a JSON checkpoint. Returns list of dicts.\"\"\"\n",
        "    if os.path.exists(filepath):\n",
        "        with open(filepath, \"r\") as f:\n",
        "            data = _json.load(f)\n",
        "        return data.get(\"results\", [])\n",
        "    return []\n",
        "\n",
        "\n",
        "def _save_grid_checkpoint(filepath, results, grid):\n",
        "    \"\"\"Save results and grid definition to a JSON checkpoint.\"\"\"\n",
        "    with open(filepath, \"w\") as f:\n",
        "        _json.dump({\"grid\": grid, \"results\": results}, f, indent=2)\n",
        "\n",
        "\n",
        "def _params_match(a, b, tol=1e-10):\n",
        "    \"\"\"Check if two param lists are the same (within float tolerance).\"\"\"\n",
        "    return all(abs(x - y) < tol for x, y in zip(a, b))\n",
        "\n",
        "\n",
        "# --- Resolve checkpoint ---\n",
        "grid_checkpoint_id = None\n",
        "grid_checkpoint_file = None\n",
        "grid_completed_results = []\n",
        "\n",
        "if GRID_USE_CHECKPOINT:\n",
        "    latest_id = _get_grid_checkpoint_id(GRID_CHECKPOINT_BASE_NAME, find_latest=True)\n",
        "    if latest_id is not None:\n",
        "        grid_checkpoint_id = latest_id\n",
        "        grid_checkpoint_file = os.path.join(DRIVE_DIR, f\"{GRID_CHECKPOINT_BASE_NAME}_{grid_checkpoint_id}.json\")\n",
        "        grid_completed_results = _load_grid_checkpoint(grid_checkpoint_file)\n",
        "        print(f\"Loaded grid checkpoint ID {grid_checkpoint_id} with {len(grid_completed_results)} completed trials.\")\n",
        "        if grid_completed_results:\n",
        "            best_prev = min(grid_completed_results, key=lambda r: r[\"mean_val_loss\"])\n",
        "            print(f\"  Best so far: loss={best_prev['mean_val_loss']:.4f} \"\n",
        "                  f\"(lr={best_prev['params'][0]:.6f}, wd={best_prev['params'][1]:.6f}, do={best_prev['params'][2]:.4f})\")\n",
        "    else:\n",
        "        print(\"No existing grid search checkpoints found. Starting new.\")\n",
        "\n",
        "if grid_checkpoint_id is None:\n",
        "    grid_checkpoint_id = _get_grid_checkpoint_id(GRID_CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "    grid_checkpoint_file = os.path.join(DRIVE_DIR, f\"{GRID_CHECKPOINT_BASE_NAME}_{grid_checkpoint_id}.json\")\n",
        "    print(f\"New grid search will use checkpoint ID {grid_checkpoint_id}.\")\n",
        "\n",
        "# Build set of already-completed param tuples for fast lookup\n",
        "_completed_set = set()\n",
        "for r in grid_completed_results:\n",
        "    _completed_set.add(tuple(round(v, 10) for v in r[\"params\"]))\n",
        "\n",
        "# --- Override globals so train_model() logs to a \"grid_search_X\" WandB group ---\n",
        "_saved_CHECKPOINT_BASE_NAME = CHECKPOINT_BASE_NAME\n",
        "_saved_checkpoint_id = checkpoint_id_for_this_run\n",
        "_saved_current_call = current_call\n",
        "_saved_CALLS = CALLS\n",
        "\n",
        "CHECKPOINT_BASE_NAME = GRID_CHECKPOINT_BASE_NAME\n",
        "checkpoint_id_for_this_run = grid_checkpoint_id\n",
        "current_call = len(grid_completed_results)\n",
        "CALLS = GRID_TOTAL\n",
        "\n",
        "# --- Main grid search loop ---\n",
        "remaining_grid = [\n",
        "    combo for combo in grid_combinations\n",
        "    if tuple(round(v, 10) for v in combo) not in _completed_set\n",
        "]\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  Grid Search: {len(remaining_grid)} remaining / {GRID_TOTAL} total trials\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "grid_start_time = time.time()\n",
        "best_grid_loss = min((r[\"mean_val_loss\"] for r in grid_completed_results), default=float(\"inf\"))\n",
        "\n",
        "for combo_idx, params in enumerate(remaining_grid):\n",
        "    lr, wd, do = params\n",
        "    trial_num = current_call + 1\n",
        "    print(f\"\\n>>> Grid trial {trial_num}/{GRID_TOTAL}  \"\n",
        "          f\"[lr={lr:.6f}, wd={wd:.6f}, dropout={do:.4f}]\")\n",
        "\n",
        "    mean_val_loss = train_model(params)\n",
        "\n",
        "    # Record result\n",
        "    result_entry = {\n",
        "        \"params\": [lr, wd, do],\n",
        "        \"mean_val_loss\": mean_val_loss,\n",
        "        \"trial\": trial_num,\n",
        "    }\n",
        "    grid_completed_results.append(result_entry)\n",
        "\n",
        "    # Track best\n",
        "    if mean_val_loss < best_grid_loss:\n",
        "        best_grid_loss = mean_val_loss\n",
        "        print(f\"  *** New best grid loss: {best_grid_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint after every trial\n",
        "    _save_grid_checkpoint(grid_checkpoint_file, grid_completed_results, grid_combinations)\n",
        "    print(f\"  Checkpoint saved ({len(grid_completed_results)}/{GRID_TOTAL} done).\")\n",
        "\n",
        "grid_end_time = time.time()\n",
        "\n",
        "# --- Summary ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  Grid Search Complete!\")\n",
        "print(f\"  Total time: {(grid_end_time - grid_start_time)/60:.2f} minutes\")\n",
        "print(f\"  Trials run this session: {len(remaining_grid)}\")\n",
        "print(f\"  Total completed: {len(grid_completed_results)}/{GRID_TOTAL}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "if grid_completed_results:\n",
        "    best = min(grid_completed_results, key=lambda r: r[\"mean_val_loss\"])\n",
        "    print(f\"\\n  Best Grid Search Result:\")\n",
        "    print(f\"    Learning Rate: {best['params'][0]:.6f}\")\n",
        "    print(f\"    Weight Decay:  {best['params'][1]:.6f}\")\n",
        "    print(f\"    Dropout:       {best['params'][2]:.4f}\")\n",
        "    print(f\"    Mean CV Loss:  {best['mean_val_loss']:.4f}\")\n",
        "\n",
        "    # Show all results sorted by loss\n",
        "    print(f\"\\n  All results (sorted by loss):\")\n",
        "    for i, r in enumerate(sorted(grid_completed_results, key=lambda r: r[\"mean_val_loss\"])):\n",
        "        print(f\"    {i+1:2d}. loss={r['mean_val_loss']:.4f}  \"\n",
        "              f\"lr={r['params'][0]:.6f}  wd={r['params'][1]:.6f}  do={r['params'][2]:.4f}\")\n",
        "\n",
        "# --- Restore globals ---\n",
        "CHECKPOINT_BASE_NAME = _saved_CHECKPOINT_BASE_NAME\n",
        "checkpoint_id_for_this_run = _saved_checkpoint_id\n",
        "current_call = _saved_current_call\n",
        "CALLS = _saved_CALLS\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "activeml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
