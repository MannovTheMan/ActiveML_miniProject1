{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import ConcatDataset, DataLoader, SubsetRandomSampler\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from skopt import gp_minimize, load\n",
        "from skopt.space import Real\n",
        "from skopt.callbacks import CheckpointSaver\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import re\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of c:\\Users\\JMN\\Documents\\Privat\\Uddannelse\\ActiveML\\mini-projekt\\dataset: ['Testing', 'Training']\n",
            "Loaded cached normalization stats from c:\\Users\\JMN\\Documents\\Privat\\Uddannelse\\ActiveML\\mini-projekt\\BO_Checkpoints\\dataset_norm_stats.json\n",
            "Dataset mean: [0.18654859066009521, 0.18655261397361755, 0.18659797310829163]\n",
            "Dataset std:  [0.19559581577777863, 0.19559480249881744, 0.1956312358379364]\n",
            "Total dataset size: 7200 images\n",
            "SimpleTumorCNN parameter count: 24,068\n",
            "Attempting to load latest checkpoint from c:\\Users\\JMN\\Documents\\Privat\\Uddannelse\\ActiveML\\mini-projekt\\BO_Checkpoints/3d_cv_optimization_0.pkl...\n",
            "Resuming from 26 previous calls from latest ID 0.\n",
            "  Best loss so far: 0.3018\n",
            "Starting optimization with 74 remaining calls (Total CALLS: 100)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>data_loading_pct</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>fold</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>data_loading_pct</td><td>43.35639</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>fold</td><td>1</td></tr><tr><td>train_loss</td><td>0.90513</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial_27</strong> at: <a href='https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/brain-tumor-bo-optimization/runs/cr09bcj5' target=\"_blank\">https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/brain-tumor-bo-optimization/runs/cr09bcj5</a><br> View project at: <a href='https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/brain-tumor-bo-optimization' target=\"_blank\">https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/brain-tumor-bo-optimization</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20260222_140135-cr09bcj5\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.25.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\JMN\\Documents\\Privat\\Uddannelse\\ActiveML\\mini-projekt\\wandb\\run-20260222_140217-q6k5e0p2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/q6k5e0p2' target=\"_blank\">trial_27</a></strong> to <a href='https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN' target=\"_blank\">https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/q6k5e0p2' target=\"_blank\">https://wandb.ai/2121jmmn-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/q6k5e0p2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "  Trial 27/100\n",
            "  lr=0.002074  wd=0.000205  dropout=0.0901\n",
            "============================================================\n",
            "Using device: cpu\n",
            "\n",
            "  --- Fold 1/3 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Fold 1 Epoch 22/50:  63%|██████▎   | 95/150 [00:08<00:04, 12.28it/s, loss=0.4309, epoch_est=00:12, data%=5%] "
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# Checkpoint Configuration Variables\n",
        "# ==========================================\n",
        "CHECKPOINT_BASE_NAME = '3d_cv_optimization'\n",
        "USE_CHECKPOINT = True   # Set to True to resume from a checkpoint, False to start new\n",
        "DESIRED_CHECKPOINT_ID = None  # Set to None for latest, or an integer for a specific checkpoint ID\n",
        "\n",
        "# Local checkpoint directory\n",
        "DRIVE_DIR = r\"c:\\Users\\JMN\\Documents\\Privat\\Uddannelse\\ActiveML\\mini-projekt\\BO_Checkpoints\"\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# 1. Data Preprocessing & Loading\n",
        "# ==========================================\n",
        "# Local dataset path\n",
        "dataset_path = r\"c:\\Users\\JMN\\Documents\\Privat\\Uddannelse\\ActiveML\\mini-projekt\\dataset\"\n",
        "print(f\"Contents of {dataset_path}: {os.listdir(dataset_path)}\")\n",
        "\n",
        "# --- Compute (or load cached) dataset-specific normalization statistics ---\n",
        "import json\n",
        "NORM_STATS_FILE = os.path.join(DRIVE_DIR, \"dataset_norm_stats.json\")\n",
        "\n",
        "if os.path.exists(NORM_STATS_FILE):\n",
        "    with open(NORM_STATS_FILE, \"r\") as f:\n",
        "        _stats = json.load(f)\n",
        "    DATASET_MEAN = _stats[\"mean\"]\n",
        "    DATASET_STD  = _stats[\"std\"]\n",
        "    print(f\"Loaded cached normalization stats from {NORM_STATS_FILE}\")\n",
        "else:\n",
        "    print(\"Computing dataset-specific normalization statistics (first run)...\")\n",
        "    _tmp_transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    _tmp_train = ImageFolder(os.path.join(dataset_path, \"Training\"), transform=_tmp_transform)\n",
        "    _tmp_test  = ImageFolder(os.path.join(dataset_path, \"Testing\"),  transform=_tmp_transform)\n",
        "    _tmp_all   = ConcatDataset([_tmp_train, _tmp_test])\n",
        "    _tmp_loader = DataLoader(_tmp_all, batch_size=256, shuffle=False, num_workers=0)\n",
        "\n",
        "    _mean = torch.zeros(3)\n",
        "    _std  = torch.zeros(3)\n",
        "    _n_pixels = 0\n",
        "    for imgs, _ in tqdm(_tmp_loader, desc=\"Norm stats\", leave=False):\n",
        "        b, c, h, w = imgs.shape\n",
        "        _n_pixels += b * h * w\n",
        "        _mean += imgs.sum(dim=[0, 2, 3])\n",
        "        _std  += (imgs ** 2).sum(dim=[0, 2, 3])\n",
        "\n",
        "    DATASET_MEAN = (_mean / _n_pixels).tolist()\n",
        "    DATASET_STD  = ((_std / _n_pixels - torch.tensor(DATASET_MEAN) ** 2).sqrt()).tolist()\n",
        "    del _tmp_transform, _tmp_train, _tmp_test, _tmp_all, _tmp_loader, _mean, _std, _n_pixels\n",
        "\n",
        "    # Save for future runs\n",
        "    with open(NORM_STATS_FILE, \"w\") as f:\n",
        "        json.dump({\"mean\": DATASET_MEAN, \"std\": DATASET_STD}, f, indent=2)\n",
        "    print(f\"Saved normalization stats to {NORM_STATS_FILE}\")\n",
        "\n",
        "print(f\"Dataset mean: {DATASET_MEAN}\")\n",
        "print(f\"Dataset std:  {DATASET_STD}\")\n",
        "\n",
        "# --- Final transform with computed statistics ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=DATASET_MEAN, std=DATASET_STD)\n",
        "])\n",
        "\n",
        "training_dataset = ImageFolder(os.path.join(dataset_path, \"Training\"), transform=transform)\n",
        "testing_dataset  = ImageFolder(os.path.join(dataset_path, \"Testing\"),  transform=transform)\n",
        "dataset = ConcatDataset([training_dataset, testing_dataset])\n",
        "print(f\"Total dataset size: {len(dataset)} images\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Model Definition — SimpleTumorCNN\n",
        "# ==========================================\n",
        "class SimpleTumorCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight custom CNN (~24k parameters).\n",
        "    3 conv blocks with BatchNorm, AdaptiveAvgPool, and a single FC head.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=4, dropout_rate=0.1):\n",
        "        super(SimpleTumorCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: 3 -> 16 channels\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Block 2: 16 -> 32 channels\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Block 3: 32 -> 64 channels\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Global pooling\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Verify parameter count\n",
        "_tmp_model = SimpleTumorCNN(num_classes=4, dropout_rate=0.1)\n",
        "_param_count = sum(p.numel() for p in _tmp_model.parameters())\n",
        "print(f\"SimpleTumorCNN parameter count: {_param_count:,}\")\n",
        "del _tmp_model\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ==========================================\n",
        "# 3. Training Params & BO Configuration\n",
        "# ==========================================\n",
        "CALLS = 100        # Total BO trials\n",
        "EPOCHS = 50        # Epochs per trial per fold\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 3\n",
        "N_FOLDS = 3        # 3-Fold Cross-Validation\n",
        "SEED = 42\n",
        "\n",
        "# 3D Search Space\n",
        "search_space = [\n",
        "    Real(1e-4, 1e-1, prior='log-uniform', name='learning_rate'),\n",
        "    Real(1e-5, 1e-2, prior='log-uniform', name='weight_decay'),\n",
        "    Real(0.0,  0.5,  prior='uniform',     name='dropout'),\n",
        "]\n",
        "\n",
        "# Global state for trial numbering and WandB grouping\n",
        "current_call = 0\n",
        "checkpoint_id_for_this_run = 0  # Will be set by main block; used as WandB group\n",
        "\n",
        "def get_checkpoint_id(base_name, find_latest=False):\n",
        "    \"\"\"\n",
        "    Generates a new unique ID for new runs or finds the latest existing ID for resuming.\n",
        "    \"\"\"\n",
        "    existing_ids = []\n",
        "    for f_name in os.listdir(DRIVE_DIR):\n",
        "        match = re.match(rf'^{re.escape(base_name)}_(\\d+)\\.pkl$', f_name)\n",
        "        if match:\n",
        "            existing_ids.append(int(match.group(1)))\n",
        "\n",
        "    if find_latest:\n",
        "        return max(existing_ids) if existing_ids else None\n",
        "    else:\n",
        "        if not existing_ids:\n",
        "            return 0\n",
        "        else:\n",
        "            existing_ids.sort()\n",
        "            for i, _id in enumerate(existing_ids):\n",
        "                if i != _id:\n",
        "                    return i\n",
        "            return len(existing_ids)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4. Objective Function (3-Fold CV)\n",
        "# ==========================================\n",
        "def train_model(params):\n",
        "    \"\"\"\n",
        "    Objective function for Bayesian Optimization.\n",
        "    Trains SimpleTumorCNN with 3-Fold CV and returns mean validation loss.\n",
        "    \"\"\"\n",
        "    global current_call, checkpoint_id_for_this_run\n",
        "    current_call += 1\n",
        "\n",
        "    learning_rate = params[0]\n",
        "    weight_decay  = params[1]\n",
        "    dropout       = params[2]\n",
        "\n",
        "    # Clear GPU memory from previous trial\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Initialize WandB for this trial\n",
        "    run = wandb.init(\n",
        "        entity=\"2121jmmn-danmarks-tekniske-universitet-dtu\",\n",
        "        project=\"3d_cv_simpleTumorCNN\",\n",
        "        group=f\"{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}\",\n",
        "        name=f\"trial_{current_call}\",\n",
        "        reinit=True,\n",
        "        resume=\"never\",\n",
        "        config={\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"dropout\": dropout,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"n_folds\": N_FOLDS,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"trial\": current_call,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  Trial {current_call}/{CALLS}\")\n",
        "    print(f\"  lr={learning_rate:.6f}  wd={weight_decay:.6f}  dropout={dropout:.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- 3-Fold Cross-Validation ---\n",
        "    kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_losses = []\n",
        "    fold_accuracies = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(range(len(dataset)))):\n",
        "        print(f\"\\n  --- Fold {fold_idx + 1}/{N_FOLDS} ---\")\n",
        "\n",
        "        # Samplers for this fold\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler   = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        workers = NUM_WORKERS\n",
        "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                  sampler=train_sampler,\n",
        "                                  num_workers=workers, persistent_workers=True)\n",
        "        val_loader   = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                  sampler=val_sampler,\n",
        "                                  num_workers=workers, persistent_workers=True)\n",
        "\n",
        "        # Fresh model & optimizer per fold\n",
        "        model = SimpleTumorCNN(num_classes=4, dropout_rate=dropout).to(device)\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "        # --- Training loop ---\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            data_time = 0.0\n",
        "            compute_time = 0.0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f\"  Fold {fold_idx+1} Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "            end = time.time()\n",
        "\n",
        "            for _batch_idx, (inputs, labels) in enumerate(pbar):\n",
        "                data_time += time.time() - end\n",
        "\n",
        "                comp_start = time.time()\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                compute_time += time.time() - comp_start\n",
        "\n",
        "                total_time = data_time + compute_time\n",
        "                data_pct = 100 * data_time / total_time if total_time > 0 else 0\n",
        "\n",
        "                elapsed = pbar.format_dict.get('elapsed', 0)\n",
        "                remaining = (pbar.format_dict.get('total', 1) - pbar.format_dict.get('n', 0)) \\\n",
        "                            * pbar.format_dict.get('elapsed', 0) \\\n",
        "                            / max(pbar.format_dict.get('n', 1), 1)\n",
        "                epoch_total = elapsed + remaining\n",
        "                et_min, et_sec = divmod(int(epoch_total), 60)\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'epoch_est': f'{et_min:02d}:{et_sec:02d}',\n",
        "                    'data%': f'{data_pct:.0f}%'\n",
        "                })\n",
        "                end = time.time()\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "            wandb.log({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": avg_train_loss,\n",
        "                \"data_loading_pct\": data_pct,\n",
        "            })\n",
        "\n",
        "        # --- Validation for this fold ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_fold_val_loss = val_loss / len(val_loader)\n",
        "        fold_accuracy = 100 * correct / total\n",
        "        fold_losses.append(avg_fold_val_loss)\n",
        "        fold_accuracies.append(fold_accuracy)\n",
        "\n",
        "        wandb.log({\n",
        "            \"fold\": fold_idx + 1,\n",
        "            \"fold_val_loss\": avg_fold_val_loss,\n",
        "            \"fold_val_accuracy\": fold_accuracy,\n",
        "        })\n",
        "        print(f\"  Fold {fold_idx+1} — Val Loss: {avg_fold_val_loss:.4f}, Accuracy: {fold_accuracy:.2f}%\")\n",
        "\n",
        "        # Cleanup per fold\n",
        "        del model, optimizer, train_loader, val_loader\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # --- Average across folds ---\n",
        "    mean_val_loss = float(np.mean(fold_losses))\n",
        "    mean_accuracy = float(np.mean(fold_accuracies))\n",
        "\n",
        "    wandb.log({\n",
        "        \"mean_cv_val_loss\": mean_val_loss,\n",
        "        \"mean_cv_val_accuracy\": mean_accuracy,\n",
        "    })\n",
        "\n",
        "    print(f\"\\n  Trial {current_call} finished — Mean CV Loss: {mean_val_loss:.4f}, Mean Accuracy: {mean_accuracy:.2f}%\")\n",
        "    run.finish()\n",
        "\n",
        "    return mean_val_loss\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. Checkpoint Logic & Bayesian Optimization\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    x0 = None\n",
        "    y0 = None\n",
        "    current_call = 0\n",
        "    checkpoint_id_for_this_run = None\n",
        "    checkpoint_file = None\n",
        "\n",
        "    if USE_CHECKPOINT:\n",
        "        if DESIRED_CHECKPOINT_ID is not None:\n",
        "            checkpoint_id_for_this_run = DESIRED_CHECKPOINT_ID\n",
        "            checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "\n",
        "            if os.path.exists(checkpoint_file):\n",
        "                print(f\"Attempting to load specific checkpoint from {checkpoint_file}...\")\n",
        "                try:\n",
        "                    res_loaded = load(checkpoint_file)\n",
        "                    x0 = [list(xi) for xi in res_loaded.x_iters]  # Ensure list of lists\n",
        "                    y0 = list(res_loaded.func_vals)                # Ensure plain list\n",
        "                    current_call = len(x0)\n",
        "                    best_so_far = min(y0)\n",
        "                    print(f\"Resuming from {current_call} previous calls from ID {checkpoint_id_for_this_run}.\")\n",
        "                    print(f\"  Best loss so far: {best_so_far:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: Could not load checkpoint {checkpoint_file}: {e}. Starting new.\")\n",
        "                    checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                    checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                    print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "            else:\n",
        "                print(f\"ERROR: Checkpoint file {checkpoint_file} not found. Starting new optimization.\")\n",
        "                checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "        else:\n",
        "            latest_id = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=True)\n",
        "            if latest_id is not None:\n",
        "                checkpoint_id_for_this_run = latest_id\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Attempting to load latest checkpoint from {checkpoint_file}...\")\n",
        "                try:\n",
        "                    res_loaded = load(checkpoint_file)\n",
        "                    x0 = [list(xi) for xi in res_loaded.x_iters]\n",
        "                    y0 = list(res_loaded.func_vals)\n",
        "                    current_call = len(x0)\n",
        "                    best_so_far = min(y0)\n",
        "                    print(f\"Resuming from {current_call} previous calls from latest ID {checkpoint_id_for_this_run}.\")\n",
        "                    print(f\"  Best loss so far: {best_so_far:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: Could not load checkpoint {checkpoint_file}: {e}. Starting new.\")\n",
        "                    checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                    checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                    print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "            else:\n",
        "                print(\"No existing checkpoints found. Starting new optimization.\")\n",
        "                checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "    else:\n",
        "        print(\"USE_CHECKPOINT is False. Starting a brand new optimization.\")\n",
        "        checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "        checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "        print(f\"New optimization will use checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "    if checkpoint_file is None:\n",
        "        checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "        checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "\n",
        "    checkpoint_callback = CheckpointSaver(checkpoint_file)\n",
        "    remaining_calls = max(0, CALLS - current_call)\n",
        "\n",
        "    print(f\"Starting optimization with {remaining_calls} remaining calls (Total CALLS: {CALLS})...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    if remaining_calls > 0:\n",
        "        # Resume-aware initial random points: 20 total, minus already-evaluated points\n",
        "        required_random = max(0, 20 - len(x0 if x0 is not None else []))\n",
        "\n",
        "        res = gp_minimize(\n",
        "            train_model,\n",
        "            search_space,                      # 3D: [lr, weight_decay, dropout]\n",
        "            acq_func=\"EI\",                     # Expected Improvement\n",
        "            xi=0.1,                           # Exploration bias\n",
        "            n_calls=remaining_calls,\n",
        "            n_initial_points=required_random,\n",
        "            noise=\"gaussian\",\n",
        "            random_state=SEED,\n",
        "            callback=[checkpoint_callback],\n",
        "            x0=x0,\n",
        "            y0=y0,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"All {CALLS} calls already completed based on loaded checkpoint.\")\n",
        "        if x0 is not None and y0 is not None:\n",
        "            best_idx = np.argmin(y0)\n",
        "            best_lr      = x0[best_idx][0]\n",
        "            best_wd      = x0[best_idx][1]\n",
        "            best_dropout = x0[best_idx][2]\n",
        "            best_loss    = y0[best_idx]\n",
        "\n",
        "            class MockResult:\n",
        "                def __init__(self, x, fun):\n",
        "                    self.x = x\n",
        "                    self.fun = fun\n",
        "\n",
        "            res = MockResult([best_lr, best_wd, best_dropout], best_loss)\n",
        "            print(f\"Best from checkpoint — LR: {res.x[0]:.6f}, WD: {res.x[1]:.6f}, \"\n",
        "                  f\"Dropout: {res.x[2]:.4f}, Loss: {res.fun:.4f}\")\n",
        "        else:\n",
        "            print(\"No results to display as no checkpoint was loaded and no new calls were made.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nOptimization finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
        "    if 'res' in locals():\n",
        "        print(f\"Best LR: {res.x[0]:.6f}, Best Weight Decay: {res.x[1]:.6f}, \"\n",
        "              f\"Best Dropout: {res.x[2]:.4f}, Best Loss: {res.fun:.4f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "activeml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
