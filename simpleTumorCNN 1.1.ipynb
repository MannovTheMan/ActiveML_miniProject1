{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Importing Libraries\n",
        "# ==========================================\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import ConcatDataset, DataLoader, SubsetRandomSampler\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from skopt import gp_minimize, load\n",
        "from skopt.space import Real\n",
        "from skopt.callbacks import CheckpointSaver\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import re\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Checkpoint Configuration Variables\n",
        "# ==========================================\n",
        "CHECKPOINT_BASE_NAME = '3d_cv_optimization'\n",
        "USE_CHECKPOINT = True   # Set to True to resume from a checkpoint, False to start new\n",
        "DESIRED_CHECKPOINT_ID = None  # Set to None for latest, or an integer for a specific checkpoint ID\n",
        "\n",
        "# Local checkpoint directory\n",
        "DRIVE_DIR = r\"\\BO_Checkpoints\"\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of dataset: ['Testing', 'Training']\n",
            "Loaded cached normalization stats from \\BO_Checkpoints\\dataset_norm_stats.json\n",
            "Dataset mean: [0.18654859066009521, 0.18655261397361755, 0.18659797310829163]\n",
            "Dataset std:  [0.19559581577777863, 0.19559478759765625, 0.1956312358379364]\n",
            "Total dataset size: 7200 images\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. Data Preprocessing & Loading\n",
        "# ==========================================\n",
        "# Local dataset path\n",
        "dataset_path = r\"dataset\"\n",
        "print(f\"Contents of {dataset_path}: {os.listdir(dataset_path)}\")\n",
        "\n",
        "# --- Compute (or load cached) dataset-specific normalization statistics ---\n",
        "import json\n",
        "NORM_STATS_FILE = os.path.join(DRIVE_DIR, \"dataset_norm_stats.json\")\n",
        "\n",
        "if os.path.exists(NORM_STATS_FILE):\n",
        "    with open(NORM_STATS_FILE, \"r\") as f:\n",
        "        _stats = json.load(f)\n",
        "    DATASET_MEAN = _stats[\"mean\"]\n",
        "    DATASET_STD  = _stats[\"std\"]\n",
        "    print(f\"Loaded cached normalization stats from {NORM_STATS_FILE}\")\n",
        "else:\n",
        "    print(\"Computing dataset-specific normalization statistics (first run)...\")\n",
        "    _tmp_transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    _tmp_train = ImageFolder(os.path.join(dataset_path, \"Training\"), transform=_tmp_transform)\n",
        "    _tmp_test  = ImageFolder(os.path.join(dataset_path, \"Testing\"),  transform=_tmp_transform)\n",
        "    _tmp_all   = ConcatDataset([_tmp_train, _tmp_test])\n",
        "    _tmp_loader = DataLoader(_tmp_all, batch_size=256, shuffle=False, num_workers=0)\n",
        "\n",
        "    _mean = torch.zeros(3)\n",
        "    _std  = torch.zeros(3)\n",
        "    _n_pixels = 0\n",
        "    for imgs, _ in tqdm(_tmp_loader, desc=\"Norm stats\", leave=False):\n",
        "        b, c, h, w = imgs.shape\n",
        "        _n_pixels += b * h * w\n",
        "        _mean += imgs.sum(dim=[0, 2, 3])\n",
        "        _std  += (imgs ** 2).sum(dim=[0, 2, 3])\n",
        "\n",
        "    DATASET_MEAN = (_mean / _n_pixels).tolist()\n",
        "    DATASET_STD  = ((_std / _n_pixels - torch.tensor(DATASET_MEAN) ** 2).sqrt()).tolist()\n",
        "    del _tmp_transform, _tmp_train, _tmp_test, _tmp_all, _tmp_loader, _mean, _std, _n_pixels\n",
        "\n",
        "    # Save for future runs\n",
        "    with open(NORM_STATS_FILE, \"w\") as f:\n",
        "        json.dump({\"mean\": DATASET_MEAN, \"std\": DATASET_STD}, f, indent=2)\n",
        "    print(f\"Saved normalization stats to {NORM_STATS_FILE}\")\n",
        "\n",
        "print(f\"Dataset mean: {DATASET_MEAN}\")\n",
        "print(f\"Dataset std:  {DATASET_STD}\")\n",
        "\n",
        "# --- Final transform with computed statistics ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=DATASET_MEAN, std=DATASET_STD)\n",
        "])\n",
        "\n",
        "training_dataset = ImageFolder(os.path.join(dataset_path, \"Training\"), transform=transform)\n",
        "testing_dataset  = ImageFolder(os.path.join(dataset_path, \"Testing\"),  transform=transform)\n",
        "dataset = ConcatDataset([training_dataset, testing_dataset])\n",
        "print(f\"Total dataset size: {len(dataset)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data preprocessing & loading**\n",
        "\n",
        "Denne kode indlæser et lokalt billeddataset (opdelt i Training/ og Testing/) og bygger en ensartet preprocessing-pipeline til en CNN. Først beregnes dataset-specifikke normaliseringsparametre (middelværdi og standardafvigelse pr. farvekanal) på tværs af hele datasættet. Hvis disse statistikker allerede er beregnet, indlæses de i stedet fra en cache-fil (dataset_norm_stats.json) for at undgå unødvendig gentagelse ved næste kørsel.\n",
        "\n",
        "Når statistikkerne er tilgængelige, defineres en endelig transform-pipeline, der:\n",
        "\n",
        "1. resizer alle billeder til 128×128,\n",
        "2. konverterer dem til PyTorch tensors (værdier i 0,1), og \n",
        "3. normaliserer billederne med datasetets egen mean/std.\n",
        "\n",
        "Til sidst indlæses både trænings- og testmappen via ImageFolder, og de samles i ét samlet datasæt med ConcatDataset, så man kan arbejde med hele mængden samlet (fx til senere splits eller cross-validation).\n",
        "\n",
        "**Nøglekoncepter**\n",
        "\n",
        "- Transform pipeline (torchvision.transforms): En kæde af deterministiske preprocessing-trin, der sikrer at alle input har samme format og skala.\n",
        "\n",
        "- Normalisering (Normalize): Standardiserer hver kanal (R,G,B) til omtrent nul-mean og enheds-std. Det stabiliserer og accelererer ofte træning, fordi inputdistributionen bliver mere “velopført” for gradient-baseret optimering.\n",
        "\n",
        "- Dataset-specifik statistik: Mean/std beregnes på jeres data (ikke ImageNet-standard), hvilket kan være vigtigt i medicinske billeder, hvor intensitetsfordelingen ofte afviger fra naturlige billeder.\n",
        "\n",
        "- Caching: Statistikker gemmes i en JSON-fil, så preprocessing bliver reproducerbar og hurtigere på efterfølgende runs.\n",
        "\n",
        "- ImageFolder + mappestruktur: ImageFolder forventer undermapper pr. klasse, og laver automatisk labels ud fra mappenavne.\n",
        "\n",
        "- Batch-wise beregning via DataLoader: Mean/std beregnes effektivt over batches for at håndtere store datasæt uden at loade alt i hukommelsen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SimpleTumorCNN parameter count: 24,068\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 2. Model Definition — SimpleTumorCNN\n",
        "# ==========================================\n",
        "class SimpleTumorCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight custom CNN (~24k parameters).\n",
        "    3 conv blocks with BatchNorm, AdaptiveAvgPool, and a single FC head.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=4, dropout_rate=0.1):\n",
        "        super(SimpleTumorCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: 3 -> 16 channels\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Block 2: 16 -> 32 channels\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Block 3: 32 -> 64 channels\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            # Global pooling\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Verify parameter count\n",
        "_tmp_model = SimpleTumorCNN(num_classes=4, dropout_rate=0.1)\n",
        "_param_count = sum(p.numel() for p in _tmp_model.parameters())\n",
        "print(f\"SimpleTumorCNN parameter count: {_param_count:,}\")\n",
        "del _tmp_model\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Modeldefinition — SimpleTumorCNN**\n",
        "\n",
        "Denne kode definerer en lille specialdesignet Convolutional Neural Network (CNN) til billedklassifikation. Modellen består af tre konvolutionsblokke efterfulgt af global pooling og et enkelt fuldt forbundet klassifikationslag. Arkitekturen er bevidst “lightweight” (ca. 24k parametre), så træning og hyperparametertuning kan gennemføres hurtigt inden for et begrænset beregningsbudget.\n",
        "\n",
        "Hver konvolutionsblok udfører:\n",
        "\n",
        "- en 2D-konvolution (feature extraction),\n",
        "\n",
        "- Batch Normalization (stabiliserer træning),\n",
        "\n",
        "- ReLU-aktivering (introducerer ikke-linearitet),\n",
        "\n",
        "- Max Pooling (nedskalerer rumlige dimensioner og øger robusthed).\n",
        "\n",
        "Efter tredje blok anvendes Adaptive Average Pooling til at komprimere feature maps til størrelse (1×1) uanset inputstørrelse. Dermed får modellen en fast featurevektor med længde 64, som sendes til klassifikationshovedet: Flatten → Dropout → Linear. Dropout-rate er en hyperparameter, der kan justeres for at reducere overfitting. Til sidst udskrives antallet af modelparametre som et sanity check.\n",
        "\n",
        "Koden afslutter med at definere tabsfunktionen CrossEntropyLoss, som er standard til multi-class klassifikation (her antages num_classes=4).\n",
        "\n",
        "**Nøglekoncepter**\n",
        "\n",
        "- Convolution (Conv2d): Lærer lokale filtre, der kan detektere mønstre som kanter, teksturer og højere niveau features.\n",
        "\n",
        "- Batch Normalization: Normaliserer aktiveringer pr. batch og gør træning mere stabil og ofte hurtigere.\n",
        "\n",
        "- ReLU: Simpel ikke-lineær aktivering, der hjælper netværket med at lære komplekse beslutningsgrænser.\n",
        "\n",
        "- Max Pooling: Reducerer rumlig opløsning (downsampling) og gør repræsentationer mere robuste over for små forskydninger.\n",
        "\n",
        "- AdaptiveAvgPool2d: “Global pooling” der sikrer fast outputstørrelse og reducerer behovet for store fuldt forbundne lag.\n",
        "\n",
        "- Dropout: Regularisering ved tilfældigt at “slukke” neuroner under træning, hvilket kan mindske overfitting.\n",
        "\n",
        "- CrossEntropyLoss: Kombinerer softmax + negativ log-likelihood og er standard loss til klassifikation med flere klasser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. Training Params & BO Configuration\n",
        "# ==========================================\n",
        "CALLS = 100        # Total BO trials\n",
        "EPOCHS = 50        # Epochs per trial per fold\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 3\n",
        "N_FOLDS = 3        # 3-Fold Cross-Validation\n",
        "SEED = 42\n",
        "\n",
        "# 3D Search Space\n",
        "search_space = [\n",
        "    Real(1e-4, 1e-1, prior='log-uniform', name='learning_rate'),\n",
        "    Real(1e-5, 1e-2, prior='log-uniform', name='weight_decay'),\n",
        "    Real(0.0,  0.5,  prior='uniform',     name='dropout'),\n",
        "]\n",
        "\n",
        "# Global state for trial numbering and WandB grouping\n",
        "current_call = 0\n",
        "checkpoint_id_for_this_run = 0  # Will be set by main block; used as WandB group\n",
        "\n",
        "def get_checkpoint_id(base_name, find_latest=False):\n",
        "    \"\"\"\n",
        "    Generates a new unique ID for new runs or finds the latest existing ID for resuming.\n",
        "    \"\"\"\n",
        "    existing_ids = []\n",
        "    for f_name in os.listdir(DRIVE_DIR):\n",
        "        match = re.match(rf'^{re.escape(base_name)}_(\\d+)\\.pkl$', f_name)\n",
        "        if match:\n",
        "            existing_ids.append(int(match.group(1)))\n",
        "\n",
        "    if find_latest:\n",
        "        return max(existing_ids) if existing_ids else None\n",
        "    else:\n",
        "        if not existing_ids:\n",
        "            return 0\n",
        "        else:\n",
        "            existing_ids.sort()\n",
        "            for i, _id in enumerate(existing_ids):\n",
        "                if i != _id:\n",
        "                    return i\n",
        "            return len(existing_ids)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training params & BO-konfiguration**\n",
        "\n",
        "Dette afsnit fastlægger de centrale eksperimentparametre til både CNN-træning og Bayesian Optimization (BO). Først defineres “budgettet” for optimeringen: hvor mange BO-trials der køres (CALLS=100), hvor længe hver trial trænes (EPOCHS=50), samt batch size og antal worker-tråde til data-loading. For at få en mere robust vurdering af hver hyperparameterkonfiguration anvendes 3-fold cross-validation (N_FOLDS=3), og en fast random seed sikrer reproducerbarhed.\n",
        "\n",
        "Dernæst specificeres BO’s 3-dimensionelle search space (hyperparametre, som skal optimeres):\n",
        "\n",
        "- learning_rate i intervallet $[10−4,10−1]$ på log-skala (log-uniform),\n",
        "\n",
        "- weight_decay i intervallet $[10−5,10−2]$ på log-skala (log-uniform),\n",
        "\n",
        "- dropout i intervallet $[0.0,0.5]$ på lineær skala (uniform).\n",
        "\n",
        "Log-uniform prior bruges til parametre som learning rate og weight decay, fordi relevante værdier ofte spænder over størrelsesordener, og det typisk giver bedre dækning end en lineær sampling.\n",
        "\n",
        "Til sidst opsættes global state til at holde styr på trial-nummerering samt et “run-id” (checkpoint_id_for_this_run), som kan bruges til gruppering (fx i eksperimenttracking). Funktionen get_checkpoint_id() håndterer checkpoint-versionering: den kan enten (a) finde det seneste checkpoint-id for at genoptage en tidligere kørsel eller (b) generere et nyt ledigt id ved at scanne eksisterende checkpoint-filer i DRIVE_DIR.\n",
        "\n",
        "**Nøglekoncepter**\n",
        "\n",
        "- BO budget (CALLS): Antal gange objective-funktionen evalueres (dvs. antal hyperparameterforsøg).\n",
        "\n",
        "- Trial cost (EPOCHS × folds): Hver trial er dyr, fordi den indebærer træning (ofte flere gange via CV).\n",
        "\n",
        "- K-fold cross-validation: Giver mere stabil performance-estimat end et enkelt train/val split, men øger compute.\n",
        "\n",
        "- Search space + priors: Definerer hvilke hyperparametre BO må prøve og hvordan de samples (log-uniform vs uniform).\n",
        "\n",
        "- Reproducerbarhed (SEED): Reducerer tilfældige variationer mellem runs.\n",
        "\n",
        "- Checkpointing/versionering: Muliggør at eksperimenter kan genoptages og organiseres systematisk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 4. Objective Function (3-Fold CV)\n",
        "# ==========================================\n",
        "def train_model(params):\n",
        "    \"\"\"\n",
        "    Objective function for Bayesian Optimization.\n",
        "    Trains SimpleTumorCNN with 3-Fold CV and returns mean validation loss.\n",
        "    \"\"\"\n",
        "    global current_call, checkpoint_id_for_this_run\n",
        "    current_call += 1\n",
        "\n",
        "    learning_rate = params[0]\n",
        "    weight_decay  = params[1]\n",
        "    dropout       = params[2]\n",
        "\n",
        "    # Clear GPU memory from previous trial\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Initialize WandB for this trial\n",
        "    run = wandb.init(\n",
        "        #entity=\"2121jmmn-danmarks-tekniske-universitet-dtu\",\n",
        "        project=\"3d_cv_simpleTumorCNN\",\n",
        "        group=f\"{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}\",\n",
        "        name=f\"trial_{current_call}\",\n",
        "        reinit=True,\n",
        "        resume=\"never\",\n",
        "        config={\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"dropout\": dropout,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"n_folds\": N_FOLDS,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"trial\": current_call,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  Trial {current_call}/{CALLS}\")\n",
        "    print(f\"  lr={learning_rate:.6f}  wd={weight_decay:.6f}  dropout={dropout:.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- 3-Fold Cross-Validation ---\n",
        "    kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_losses = []\n",
        "    fold_accuracies = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(range(len(dataset)))):\n",
        "        print(f\"\\n  --- Fold {fold_idx + 1}/{N_FOLDS} ---\")\n",
        "\n",
        "        # Samplers for this fold\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler   = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        workers = NUM_WORKERS\n",
        "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                  sampler=train_sampler,\n",
        "                                  num_workers=workers, persistent_workers=True)\n",
        "        val_loader   = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                  sampler=val_sampler,\n",
        "                                  num_workers=workers, persistent_workers=True)\n",
        "\n",
        "        # Fresh model & optimizer per fold\n",
        "        model = SimpleTumorCNN(num_classes=4, dropout_rate=dropout).to(device)\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "        # --- Training loop ---\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            data_time = 0.0\n",
        "            compute_time = 0.0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f\"  Fold {fold_idx+1} Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "            end = time.time()\n",
        "\n",
        "            for _batch_idx, (inputs, labels) in enumerate(pbar):\n",
        "                data_time += time.time() - end\n",
        "\n",
        "                comp_start = time.time()\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                compute_time += time.time() - comp_start\n",
        "\n",
        "                total_time = data_time + compute_time\n",
        "                data_pct = 100 * data_time / total_time if total_time > 0 else 0\n",
        "\n",
        "                elapsed = pbar.format_dict.get('elapsed', 0)\n",
        "                remaining = (pbar.format_dict.get('total', 1) - pbar.format_dict.get('n', 0)) \\\n",
        "                            * pbar.format_dict.get('elapsed', 0) \\\n",
        "                            / max(pbar.format_dict.get('n', 1), 1)\n",
        "                epoch_total = elapsed + remaining\n",
        "                et_min, et_sec = divmod(int(epoch_total), 60)\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'epoch_est': f'{et_min:02d}:{et_sec:02d}',\n",
        "                    'data%': f'{data_pct:.0f}%'\n",
        "                })\n",
        "                end = time.time()\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "            wandb.log({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": avg_train_loss,\n",
        "                \"data_loading_pct\": data_pct,\n",
        "            })\n",
        "\n",
        "        # --- Validation for this fold ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_fold_val_loss = val_loss / len(val_loader)\n",
        "        fold_accuracy = 100 * correct / total\n",
        "        fold_losses.append(avg_fold_val_loss)\n",
        "        fold_accuracies.append(fold_accuracy)\n",
        "\n",
        "        wandb.log({\n",
        "            \"fold\": fold_idx + 1,\n",
        "            \"fold_val_loss\": avg_fold_val_loss,\n",
        "            \"fold_val_accuracy\": fold_accuracy,\n",
        "        })\n",
        "        print(f\"  Fold {fold_idx+1} — Val Loss: {avg_fold_val_loss:.4f}, Accuracy: {fold_accuracy:.2f}%\")\n",
        "\n",
        "        # Cleanup per fold\n",
        "        del model, optimizer, train_loader, val_loader\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # --- Average across folds ---\n",
        "    mean_val_loss = float(np.mean(fold_losses))\n",
        "    mean_accuracy = float(np.mean(fold_accuracies))\n",
        "\n",
        "    wandb.log({\n",
        "        \"mean_cv_val_loss\": mean_val_loss,\n",
        "        \"mean_cv_val_accuracy\": mean_accuracy,\n",
        "    })\n",
        "\n",
        "    print(f\"\\n  Trial {current_call} finished — Mean CV Loss: {mean_val_loss:.4f}, Mean Accuracy: {mean_accuracy:.2f}%\")\n",
        "    run.finish()\n",
        "\n",
        "    return mean_val_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Objective Function — 3-Fold Cross-Validation**\n",
        "\n",
        "Denne funktion definerer den objective function, som Bayesian Optimization skal minimere. For en given hyperparameterkonfiguration (learning_rate, weight_decay, dropout) trænes modellen ved hjælp af 3-fold cross-validation, og den gennemsnitlige validerings-loss returneres som optimeringsmål.\n",
        "\n",
        "For hver BO-trial:\n",
        "\n",
        "1. Hyperparametrene udtrækkes fra search space.\n",
        "\n",
        "2. GPU-hukommelse ryddes for at undgå memory leaks mellem trials.\n",
        "\n",
        "3. En ny eksperimentkørsel initialiseres i Weights & Biases (WandB) til logging.\n",
        "\n",
        "4. Datasættet opdeles i 3 folds via KFold, hvor hver fold på skift fungerer som valideringssæt.\n",
        "\n",
        "For hver fold:\n",
        "\n",
        "- En ny model og optimizer (AdamW) initialiseres.\n",
        "\n",
        "- Modellen trænes i EPOCHS epoker.\n",
        "\n",
        "- Træningsloss logges løbende.\n",
        "\n",
        "- Efter træning evalueres modellen på valideringssættet.\n",
        "\n",
        "- Foldens validerings-loss og accuracy gemmes.\n",
        "\n",
        "Til sidst beregnes middelværdien af validerings-loss og accuracy over alle folds. Den gennemsnitlige cross-validation loss returneres som funktionens output — dette er den værdi, som Bayesian Optimization forsøger at minimere.\n",
        "\n",
        "**Nøglekoncepter**\n",
        "\n",
        "- Black-box objective: Hver evaluering indebærer fuld modeltræning og validering, hvilket gør funktionen dyr og støjende.\n",
        "\n",
        "- K-fold cross-validation: Reducerer variance i performance-estimatet og giver en mere robust evaluering af hyperparametre.\n",
        "\n",
        "- Reinitialisering per fold: Forhindrer informationslækage mellem folds og sikrer fair evaluering.\n",
        "\n",
        "- AdamW optimizer: Variant af Adam med decoupled weight decay, ofte mere stabil ved regularisering.\n",
        "\n",
        "- GPU memory management: torch.cuda.empty_cache() reducerer risiko for out-of-memory fejl ved mange BO-trials.\n",
        "\n",
        "- Eksperimenttracking (WandB): Muliggør systematisk logging af hyperparametre og performance pr. trial.\n",
        "\n",
        "- Mean CV loss som objective: BO optimerer validerings-loss frem for accuracy, da loss typisk giver en mere sensitiv og differentierbar målfunktion.\n",
        "\n",
        "<span style=\"color:red\"><i>Dette er præcis det punkt, hvor vores problem bliver et klassisk dyrt black-box optimeringsproblem, hvilket gør det velegnet til Bayesian Optimization.</i></span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No existing checkpoints found. Starting new optimization.\n",
            "Starting new optimization with checkpoint ID 0.\n",
            "Starting optimization with 100 remaining calls (Total CALLS: 100)...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.25.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\veren\\Documents\\DTU 2026\\Active machine learning and agency\\Miniproject 1\\wandb\\run-20260224_083050-4k9mej2h</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/4k9mej2h' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN' target=\"_blank\">https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/4k9mej2h' target=\"_blank\">https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/4k9mej2h</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "  Trial 1/100\n",
            "  lr=0.024526  wd=0.000036  dropout=0.3898\n",
            "============================================================\n",
            "Using device: cpu\n",
            "\n",
            "  --- Fold 1/3 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Fold 1 — Val Loss: 0.3012, Accuracy: 89.46%\n",
            "\n",
            "  --- Fold 2/3 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Fold 2 — Val Loss: 0.3294, Accuracy: 88.17%\n",
            "\n",
            "  --- Fold 3/3 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Fold 3 — Val Loss: 0.2611, Accuracy: 91.33%\n",
            "\n",
            "  Trial 1 finished — Mean CV Loss: 0.2973, Mean Accuracy: 89.65%\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>data_loading_pct</td><td>▂▅▄▆▅▆▅▅▄▅▁▃▇▄▄▄▄▄▃▂▆▅▄▇▅▃█▃▄▄▆▄▃▄▅▄▅▄▃▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▄▄▅▅▅▆▆██▁▂▂▃▃▆▆▆███▁▁▂▃▄▆▆▆▇▇██</td></tr><tr><td>fold</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅███████████████</td></tr><tr><td>fold_val_accuracy</td><td>▄▁█</td></tr><tr><td>fold_val_loss</td><td>▅█▁</td></tr><tr><td>mean_cv_val_accuracy</td><td>▁</td></tr><tr><td>mean_cv_val_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>▆▅▅▅▄▄▃▄▃▃▂▂▂▁▁▅▅▄▃▃▃▃▃▃▃▂▂▂▂▂▁█▇▆▄▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>data_loading_pct</td><td>2.91657</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>fold</td><td>3</td></tr><tr><td>fold_val_accuracy</td><td>91.33333</td></tr><tr><td>fold_val_loss</td><td>0.26113</td></tr><tr><td>mean_cv_val_accuracy</td><td>89.65278</td></tr><tr><td>mean_cv_val_loss</td><td>0.29727</td></tr><tr><td>train_loss</td><td>0.30154</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/4k9mej2h' target=\"_blank\">https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/4k9mej2h</a><br> View project at: <a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN' target=\"_blank\">https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20260224_083050-4k9mej2h\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.25.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\veren\\Documents\\DTU 2026\\Active machine learning and agency\\Miniproject 1\\wandb\\run-20260224_095133-kpyyujib</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/kpyyujib' target=\"_blank\">trial_2</a></strong> to <a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN' target=\"_blank\">https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/kpyyujib' target=\"_blank\">https://wandb.ai/verena_vanessa-danmarks-tekniske-universitet-dtu/3d_cv_simpleTumorCNN/runs/kpyyujib</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "  Trial 2/100\n",
            "  lr=0.006174  wd=0.000218  dropout=0.0500\n",
            "============================================================\n",
            "Using device: cpu\n",
            "\n",
            "  --- Fold 1/3 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                               \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_calls > \u001b[32m0\u001b[39m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# Resume-aware initial random points: 20 total, minus already-evaluated points\u001b[39;00m\n\u001b[32m     80\u001b[39m     required_random = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[32m20\u001b[39m - \u001b[38;5;28mlen\u001b[39m(x0 \u001b[38;5;28;01mif\u001b[39;00m x0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []))\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     res = \u001b[43mgp_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# 3D: [lr, weight_decay, dropout]\u001b[39;49;00m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEI\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Expected Improvement\u001b[39;49;00m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\u001b[38;5;66;43;03m# Exploration bias\u001b[39;49;00m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremaining_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgaussian\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43my0\u001b[49m\u001b[43m=\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCALLS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m calls already completed based on loaded checkpoint.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\gp.py:281\u001b[39m, in \u001b[36mgp_minimize\u001b[39m\u001b[34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    274\u001b[39m     base_estimator = cook_estimator(\n\u001b[32m    275\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGP\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m         space=space,\n\u001b[32m    277\u001b[39m         random_state=rng.randint(\u001b[32m0\u001b[39m, np.iinfo(np.int32).max),\n\u001b[32m    278\u001b[39m         noise=noise,\n\u001b[32m    279\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_random_starts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_random_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[43m=\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace_constraint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspace_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_queue_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\base.py:332\u001b[39m, in \u001b[36mbase_minimize\u001b[39m\u001b[34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_calls):\n\u001b[32m    331\u001b[39m     next_x = optimizer.ask()\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     next_y = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     result = optimizer.tell(next_x, next_y)\n\u001b[32m    334\u001b[39m     result.specs = specs\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     86\u001b[39m inputs, labels = inputs.to(device), labels.to(device)\n\u001b[32m     88\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     91\u001b[39m loss.backward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mSimpleTumorCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.classifier(x)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[39m, in \u001b[36mMaxPool2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\veren\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:830\u001b[39m, in \u001b[36m_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    829\u001b[39m     stride = torch.jit.annotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 5. Checkpoint Logic & Bayesian Optimization\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    x0 = None\n",
        "    y0 = None\n",
        "    current_call = 0\n",
        "    checkpoint_id_for_this_run = None\n",
        "    checkpoint_file = None\n",
        "\n",
        "    if USE_CHECKPOINT:\n",
        "        if DESIRED_CHECKPOINT_ID is not None:\n",
        "            checkpoint_id_for_this_run = DESIRED_CHECKPOINT_ID\n",
        "            checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "\n",
        "            if os.path.exists(checkpoint_file):\n",
        "                print(f\"Attempting to load specific checkpoint from {checkpoint_file}...\")\n",
        "                try:\n",
        "                    res_loaded = load(checkpoint_file)\n",
        "                    x0 = [list(xi) for xi in res_loaded.x_iters]  # Ensure list of lists\n",
        "                    y0 = list(res_loaded.func_vals)                # Ensure plain list\n",
        "                    current_call = len(x0)\n",
        "                    best_so_far = min(y0)\n",
        "                    print(f\"Resuming from {current_call} previous calls from ID {checkpoint_id_for_this_run}.\")\n",
        "                    print(f\"  Best loss so far: {best_so_far:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: Could not load checkpoint {checkpoint_file}: {e}. Starting new.\")\n",
        "                    checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                    checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                    print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "            else:\n",
        "                print(f\"ERROR: Checkpoint file {checkpoint_file} not found. Starting new optimization.\")\n",
        "                checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "        else:\n",
        "            latest_id = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=True)\n",
        "            if latest_id is not None:\n",
        "                checkpoint_id_for_this_run = latest_id\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Attempting to load latest checkpoint from {checkpoint_file}...\")\n",
        "                try:\n",
        "                    res_loaded = load(checkpoint_file)\n",
        "                    x0 = [list(xi) for xi in res_loaded.x_iters]\n",
        "                    y0 = list(res_loaded.func_vals)\n",
        "                    current_call = len(x0)\n",
        "                    best_so_far = min(y0)\n",
        "                    print(f\"Resuming from {current_call} previous calls from latest ID {checkpoint_id_for_this_run}.\")\n",
        "                    print(f\"  Best loss so far: {best_so_far:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: Could not load checkpoint {checkpoint_file}: {e}. Starting new.\")\n",
        "                    checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                    checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                    print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "            else:\n",
        "                print(\"No existing checkpoints found. Starting new optimization.\")\n",
        "                checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "                checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "                print(f\"Starting new optimization with checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "    else:\n",
        "        print(\"USE_CHECKPOINT is False. Starting a brand new optimization.\")\n",
        "        checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "        checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "        print(f\"New optimization will use checkpoint ID {checkpoint_id_for_this_run}.\")\n",
        "\n",
        "    if checkpoint_file is None:\n",
        "        checkpoint_id_for_this_run = get_checkpoint_id(CHECKPOINT_BASE_NAME, find_latest=False)\n",
        "        checkpoint_file = f'{DRIVE_DIR}/{CHECKPOINT_BASE_NAME}_{checkpoint_id_for_this_run}.pkl'\n",
        "\n",
        "    checkpoint_callback = CheckpointSaver(checkpoint_file)\n",
        "    remaining_calls = max(0, CALLS - current_call)\n",
        "\n",
        "    print(f\"Starting optimization with {remaining_calls} remaining calls (Total CALLS: {CALLS})...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    if remaining_calls > 0:\n",
        "        # Resume-aware initial random points: 20 total, minus already-evaluated points\n",
        "        required_random = max(0, 20 - len(x0 if x0 is not None else []))\n",
        "\n",
        "        res = gp_minimize(\n",
        "            train_model,\n",
        "            search_space,                      # 3D: [lr, weight_decay, dropout]\n",
        "            acq_func=\"EI\",                     # Expected Improvement\n",
        "            xi=0.1,                           # Exploration bias\n",
        "            n_calls=remaining_calls,\n",
        "            n_initial_points=required_random,\n",
        "            noise=\"gaussian\",\n",
        "            random_state=SEED,\n",
        "            callback=[checkpoint_callback],\n",
        "            x0=x0,\n",
        "            y0=y0,\n",
        "        )\n",
        "    else:\n",
        "        print(f\"All {CALLS} calls already completed based on loaded checkpoint.\")\n",
        "        if x0 is not None and y0 is not None:\n",
        "            best_idx = np.argmin(y0)\n",
        "            best_lr      = x0[best_idx][0]\n",
        "            best_wd      = x0[best_idx][1]\n",
        "            best_dropout = x0[best_idx][2]\n",
        "            best_loss    = y0[best_idx]\n",
        "\n",
        "            class MockResult:\n",
        "                def __init__(self, x, fun):\n",
        "                    self.x = x\n",
        "                    self.fun = fun\n",
        "\n",
        "            res = MockResult([best_lr, best_wd, best_dropout], best_loss)\n",
        "            print(f\"Best from checkpoint — LR: {res.x[0]:.6f}, WD: {res.x[1]:.6f}, \"\n",
        "                  f\"Dropout: {res.x[2]:.4f}, Loss: {res.fun:.4f}\")\n",
        "        else:\n",
        "            print(\"No results to display as no checkpoint was loaded and no new calls were made.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nOptimization finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
        "    if 'res' in locals():\n",
        "        print(f\"Best LR: {res.x[0]:.6f}, Best Weight Decay: {res.x[1]:.6f}, \"\n",
        "              f\"Best Dropout: {res.x[2]:.4f}, Best Loss: {res.fun:.4f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
